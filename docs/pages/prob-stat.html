<html><head><meta charset="UTF-8" /><title>prob-stat | Jake Chvatal</title><meta content="width=device-width,initial-scale=1.0" name="viewport" /><meta content="prob-stat" property="og:title" /><meta content="website" property="og:type" /><meta content="https://jake.isnt.online" property="og:url" /><meta content="Jake Chvatal" property="og:site_name" /><meta content="hi" name="description" /><meta content="Operating Systems, webring, programming, languages" name="keywords" /><meta content="Jake Chvatal" name="author" /><meta content="index,follow" name="robots" /><meta content="#fff" name="theme-color" /><link href="/style.css" rel="stylesheet" /><script src="/lib.js"></script></head><body><div class="site-body"><div class="sidebar"><a href="/">jake.</a><a href="https://isnt.online"> ~ </a><span> / </span><a href="./index.html">pages</a><span> / </span><b>prob-stat</b></div><main><article class="wikipage"><h1 class="title-top">prob-stat</h1><div><p>Notes for reviewing my probability and statistics course atNortheastern.</p><h1 id="sample-spaces">Sample Spaces</h1><p>Experiment<br />Repeatable procedure with a set of possible results</p><p>Sample Outcome<br />One of many possible results of an experiment</p><p>Sample Space<br />$S$: The set of all possible outcomes of an experiment</p><p>Event<br />$A \subset S$ : 0 or more outcomes of an experiment</p><p>Probability<br />$P(A) = \frac{n(A)}{n(S)}$</p><h1 id="set-theory">Set Theory</h1><p>Discrete Set<br />A finite or countable set. i.e. tossing a coin until a head is received:$S = \{ H, TH, TTH, \dots \}$</p><p>Continuous Set<br />A range of values. I.e. getting a number smaller than 1 from within areal number between 0 and $\sqrt{2}$: $S = $, $A = [0, 1)$</p><p>Intersection<br />$A \cap B = \{x | x \in A\ and\ x \in B\}$ for some events A, B</p><p>Disjoint<br />$A \cap B = \emptyset$. Also known as "mutually exclusive"</p><p>Union<br />$A \cup B = \{x | x \in A\ or\ x \in B\}$</p><p>Complement<br />$A^{c} = \{x \in S | x \not{\in} A\}$</p><h2 id="demorgans-laws">DeMorgan's Law(s)</h2><ol><li>$(A \cup B)^{c} = A^{c} \cap B^{c}$</li><li>$(A \cap B)^{c} = A^{c} \cup B^{c}$</li></ol><h1 id="probability-function">Probability Function</h1><p>$P$ assigns a real number to any event of a sample space, and followsthe following axioms for a sample space that's finite:</p><ul><li>$P(A) \geq 0$ for all $A$</li><li>$P(S) = 1$</li><li>$A \cap B = \emptyset \implies P(A \cup B) = P(A) + P(B)$, or$P(A \cup B) = P(A) + P(B) - P(A \cap B)$ otherwise</li><li>$P(\cup_{i=1}^{\infty} A_{i}) = \sum_{i=1}^{\infty} P(A_{i})$ if any$A_1, A_2, A_3 \dots$ are mutually exclusive in $S$</li></ul><h1 id="conditional-probability">Conditional Probability</h1><p>$P(A|B) = \frac{P(A \cup B)}{P(B)}$$P(A \cap B) = P(B|A)P(A) = P(A|B)P(B)$ Solving conditional probability:Make a tree! Fork at each choice, recording the probability on each"leaf". Then use the leaves to assess a series of events.</p><h1 id="independence">Independence</h1><p>$A$ and $B$ are <u>independent</u> if $P(A \cap B) = P(A)P(B)$; putanother way, $P(A|B) = P(A) \iff P(B|A) = P(B)$ For more than two sets:</p><ul><li>$P(A \cap B \cap C) = P(A)P(B)P(C)$</li><li>$P(A \cap B) = P(A)P(B)$, $P(A \cap C) = P(A)P(C)$,$P(B \cap C) = P(B)P(C)$ and so on…</li></ul><h1 id="series">Series</h1><h2 id="geometric">Geometric</h2><p>$S_{n} = \sum_{k=0}^{n}r^{k} = 1 + r + r^{2} + \dots + r^{n}$ For$-1 < r < 1$, the sum converges:$S \equiv S_{\infty} = \sum_{k=0}^{\infty}r^{k} = \frac{1}{1 - r}$, for$|r| < 1$</p><p><a href="#pdf">PDF</a>$p_{Y}(k) = (1 - p)^{k - 1}p$</p><p>mean<br />$\frac{1}{p}$</p><h1 id="probability-distributions">Probability Distributions</h1><p>$(^{n}_{r}) = <em>{n}C</em>{r} = \frac{n!}{(n - r)! r!}$</p><h2 id="binomial-distribution">Binomial Distribution</h2><p>Given n <u>independent</u> trials with two outcomes and a constantP(success) for each outcome, $P(k) = (^{n}_{k}) * p^{k}(1 - p)^{n - k}$for $k = 0, 1, 2, \dots, n$.</p><p><a href="#pdf">PDF</a>$P(X = k) = (^{n}_{k}) * p^{k}(1 - p)^{n - k}$</p><ul><li>calculate: use <code>binompdf</code></li></ul><p><a href="#cdf">CDF</a>$P(X \leq t) = \sum_{k=0}^{t}(^{n}_{k}) * p^{k}(1 - p)^{n - k}$</p><ul><li>calculate: use <code>binomcdf</code></li></ul><p><a href="#expected-value">Expected Value</a>$E(X) = np$</p><h2 id="bernoulli-distribution">Bernoulli Distribution</h2><p>Essentially a <a href="#binomial-distribution">Binomial Distribution</a> where$n = 1$. $P(k) = (^{1}_{k}) * p^{k}(1 - p)^{1 - k}$</p><h2 id="hypergeometric-distribution">Hypergeometric Distribution</h2><p>$P(x) = \frac{(^{k}<em>{x})(^{N-k}</em>{n-x})}{(^{N}_{n})}$</p><ul><li>Random selection of $n$ items without replacement from a set of $N$items</li><li>Not guaranteed P(success) stays constant.</li><li>$k$ items are success; $N - k$ items are failures.</li></ul><p>This can be considered a generalization of the (#Binomial Distribution).</p><h2 id="uniform-distribution">Uniform Distribution</h2><p>All values in a range are equally likely. For some interval $$:</p><p><a href="#pdf">PDF</a>$\frac{1}{b - a}$</p><p><a href="#cdf">CDF</a>$\frac{x - a}{b - a}$</p><h2 id="exponential-distribution">Exponential Distribution</h2><p><a href="#pdf">PDF</a>$f_{X}(x) = \lambda e^{-\lambda x}$ for $x \geq 0$</p><p><a href="#cdf">CDF</a>$F_{X}(x) = 1 - e^{-\lambda x}$</p><p>mean<br />$E(X) = \frac{1}{\lambda}$</p><p>var<br />$Var(X) = \frac{1}{\lambda^{2}}$</p><h2 id="poisson-distribution">Poisson Distribution</h2><p>Counts the number of occurences per unit of measurement - over aspecific period of time, a specific area, or volume, etc…</p><p>The probability of an event occuring in a unit of measurement must bethe same for all similar units; for example, if the unit of measurementis a month, then the probability must be the same for all months.</p><p>Poisson is often used to approximate the binomial distribution: giventhat $n$ is large ($n \geq 100$) and $p$ is small, we can let \$λ =np\$! In other words, $\lambda := np$ -\> (average number of occurencesper unit) \* (length of observation period)</p><p>Use <code>poissonpdf</code> and <code>poissoncdf</code> calculator unctions to approximatethis.</p><p><a href="#pdf">PDF</a>$p_{X}(k) = P(X = k) := \frac{\lambda^{k}e^{-\lambda}}{k!}$</p><p>mean<br />$E(X) = \lambda$</p><p>variance<br />$Var(X) = \lambda$</p><h2 id="normal-distribution">Normal Distribution</h2><h1 id="density-functions">Density Functions</h1><p>Discrete Random Variable<br />Some $X$ such that:</p><ul><li>$X: S \rightarrow \mathbb{R}$</li><li>$X$ is a countable subset of $\mathbb{R}$</li><li>Motivation: Constrain the sample space to a smaller sample space,using a single variable to represent each outcome we're investigating.If we're looking at pairs of numbers, for example, we only care thatthe sum of the pair is the same, so we consider (1, 2) and (2, 1) tobe the same outcome.</li></ul><p>Continuous random variable<br />Like discrete, but ranges over a continuous interval of $\mathbb{R}$instead. Two continuous random variables are <u>independent</u> if somefunctions, $g(x)$ and $h(x)$, exist such that:</p><ul><li>$f_{X,Y}(x,y) = g(x)h(y)$</li><li>$f_{X}(x) = g(x)$</li><li>$f_{Y}(y) = h(y)$</li></ul><p>In other words, one should be able to multiply the results of themarginal pdfs to produce the joint pdf for the two variables, and viceversa.</p><h2 id="pdf">PDF</h2><p>(Probability Density Function)</p><h3 id="discrete">Discrete</h3><p>For every $X$, a probability density function (pdf) looks like:$p_{x}(k) = P(X = k) := P(\{s \in S | X(s) = k\})$, where$p_{x}(k): \mathbb{R} \rightarrow \mathbb{R}$. Here, $s$ and $S$ arefrom the original sample space we're sampling from.</p><h3 id="continuous">Continuous</h3><p>Some $f_{x}(x)$ satisfying:</p><ol><li>$f_{x}(x) \geq 0$</li><li>$\int_{-\infty}^{\infty}f_{x}(x) = 1$</li></ol><p>$P(a \leq X \leq b) = \int_{b}^{a}f_{x}(x)dx$</p><h3 id="joint-pdf">Joint PDF</h3><p>$p_{X,Y}(x,y) := P(X = x, Y = y)$, satisfying:</p><ol><li>$p_{X,Y}(x,y) \geq 0$</li><li>$\sum_{all y} \sum_{all y} p_{X,Y}(x,y) = 1$</li></ol><ol><li><p>Discrete</p><p>Given the joint pdf of $X$ and $Y$, the <u>marginal</u> pdfs of $X$and $Y$ are:</p><ul><li>$p_{X}(x) = \sum_{all y}p_{X,Y}(x,y)$</li><li>$p_{Y}(y) = \sum_{all x}p_{X,Y}(x,y)$</li></ul></li><li><p>Continuous</p><p>$P((X,Y) \in R) = \int \int_{R}f_{X,Y}(x,y) dx dy$ When solving -identify the bound that's dependent on the other! It can really helpto plot out some 2D plane, then graph the relationship between thetwo continuous random variables. From this graph it's often fairlyeasy to identify, and thus estimate, the area we're investigating;this guides us to investigate what we should learn more from!Absolutely worth working through some pracice problems.</p></li></ol><h2 id="cdf">CDF</h2><p>Cumulative Distribution Function</p><h3 id="discrete">Discrete</h3><p>$F_{x}(t) = P(X \leq t) := P(\{s \in S | X(s) \leq t\})$, where$F_{x}(t): \mathbb{R} \rightarrow \mathbb{R}$. Generally,$F_{x}(t) = \int_{-\infty}^{t}p_{x}(t)$; the pdf represents theprobability of the discrete random variable being a specific value,while the cdf represents the probability of all outcomes occuring lessthan some outcome upper bound $t$.</p><h3 id="continuous">Continuous</h3><p>$F_{x}(x) = P(X \leq x) = \int_{-\infty}^{x}p_{x}(x)dx$$P(a \leq X \leq b) = F_{X}(b) - F_{X}(a)$</p><h1 id="expected-value">Expected Value</h1><p>A generalization of the concept of "average". The name's on the tin -it's a value that represents the proportionally weighted, expectedresult.</p><p>For example, if I have a 5% chance at \$100 and a 95% chance at \$20,the expected value would be $100 * 0.05 + 0.95 * 20$, so \$24.</p><h2 id="discrete">Discrete</h2><p>$E(X) = \sum_{all \ k} k * p_{X}(k)$</p><h2 id="continuous">Continuous</h2><p>$E(X) = \int_{-\infty}^{\infty} x * p_{X}(x) dx$</p><h2 id="other-properties">Other Properties</h2><p>$E(aX + bY) = aE(X) + bE(Y)$ for any random varaibles $X, Y$ and numbers$a$ and $b$. As such, if $X$ and $Y$ are <u>independent</u>, then$E(XY) = E(X)E(Y)$.</p><h2 id="sample-mean">Sample Mean</h2><p>Denoted as $\bar{X}$$\bar{X} = \frac{1}{n}(X_{1} + X_{2} + \dots + X_{n})$</p><h1 id="median">Median</h1><h2 id="discrete">Discrete</h2><p>"Middle number" of the distribution, or the average of the two middlenumbers if the cardinality is even; the standard definition.</p><h2 id="continuous">Continuous</h2><p>$m$ such that $\int_{-\infty}^{m}f_{Y}(y) dy = 0.5$. Finding the median:</p><ol><li>Integrate and substitute.</li><li>Factor in terms of and solve for $m$.</li></ol><h1 id="variance">Variance</h1><p>A measure of how far the distribution spreads from its mean.$Var(X) := E((X - \mu)^{2})$, where:</p><ul><li>$\mu = E(X)$ is the mean of $X$</li><li>$\sigma := \sqrt{Var(X)}$ is the standard deviation of $X$</li></ul><p>If $X$ and $Y$ are independent, then:$Var(aX + bY) = a^{2}Var(X) + b^{2}Var(Y)$ In general:$Var(aX + bY) = a^{2}Var(X) + b^{2}Var(Y) - 2abCov(X,Y)$</p><h2 id="covariance">Covariance</h2><p>where $Cov(X, Y)$, the <u>covariance</u> of X and Y, is:$Cov(X,Y) := E(XY) - E(X)E(Y)$ As can be assumed, if $X$ and $Y$ areindependent, then $Cov(X,Y) = 0$.</p><h2 id="correlation">Correlation</h2><p>$Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$ A measurement of<u>correlation</u>; if positive, then the two random variables increasetogether; if negative, one increases while the other decreasese and viceversa.</p><h1 id="double-integrals">Double Integrals</h1><p>Fubini's Theorem:$\int_{a}^{b} \int_{c}^{d} f(x,y) dy dx =  \int_{c}^{d} \int_{a}^{b} f(x,y) dx dy$,given $a \leq x \leq b$ and $c \leq y \leq d$ To identify the region in$\mathbb{R}^{2}$ to integrate over, use the inside first. Treat theunevaluated integral variable as a constant and just integrate withrespect to a constant. Often one variable is much easier to integratethan the other; pick the right one to use! This takes practice.</p><h1 id="todo-problems-to-practice"><span class="todo TODO">TODO</span> Problems to Practice</h1><h2 id="3">3</h2><h3 id="graphing-pdf-cdf">Graphing PDF, CDF</h3><h3 id="converting-between-the-two-esp-with-continuous-piecewise">Converting between the two, esp. with continuous piecewise</h3><h3 id="whats-with-that-variance-theorm-and-egx-practice-those-problems">what's with that variance theorm and E(g(X))? practice those problems.</h3><p>there are some good exercises for deriving expected value and varianceavailable in the textbook - review these!</p><h1 id="do-the-exam">do the exam</h1><p>3.7 3.9 4.x 5.x, especially the problem i missed on the last exam examproblems and how exactly to approach them the rest of the 6s and 7s,though i can probably wing those just</p><h1 id="types-of-problems">Types of Problems</h1><h2 id="exam-1">Exam 1</h2><h3 id="probabilities-and-sets">Probabilities and Sets</h3><p>i.e. $P(A), P(A \cup B), P(A \cap B^{c})$ -\> find something Draw thingsout as Venn diagrams to help visualize Nice properties:</p><ul><li>$P(A \cup B) = P(A \cap B^{c}) + P(A \cap B) + P(A^{c} \cap B)$</li><li>Bayes: $P(A|B) = \frac{P(A \cap B)}{P(B)}$</li></ul><h3 id="simple-probabilities">Simple Probabilities</h3><p>Generally, use some arrangement of $nCr$ ; P(at most x) = 1 - P(at leastx); vice vers</p><h3 id="conditional-probabilities-with-scenarios">Conditional Probabilities with Scenarios</h3><p>Draw a tree:</p><ul><li>At the root, no branch is chosen</li><li><p>After the root, choose what info you have more about: i.e. if you'relooking for P(Defective \| built at plant X), draw a tree with thefirst set of descendant notes representing the plant at which thething was constructed, then from there the chances that the productwas defective <u>given</u> the plant branching off of each.</p><p>From here, can use Bayes rule to fill out the tree, then find desiresdprobability.</p></li></ul><h2 id="exam-2">Exam 2</h2><p>distributions to know: binomial,</p><h3 id="finding-mean-stdev-sample-from-given-problem">Finding Mean, stdev, sample from given problem</h3><p>$u = np$ $\sigma = \sqrt{np(1-p)}$$P(X=a) = nCa * P(a)^{a}(1-P(a))^{n-a}$ \~ <code>binompdf</code> w/ n, P(a), a</p><p>Finding P in range:$P(a < X \leq b) = cdf(500, b, P(x)) - cdf(500, a, P(x))$ gives CDF forthat range!</p><h3 id="find-cdf-of-discrete-set-of-scenarios">Find cdf of discrete set of scenarios</h3><ol><li>Write out each possible scenario and associated value of randomvariable</li><li>Use P(scenario) \* (num occurences of scenario) for each randomvariable value to compute some P(X=res) for each</li><li>Assemble into table; P(X=k) for values x=1, x=2, x=3 for example.</li></ol><h3 id="pdf---cdf">PDF -\> CDF</h3><p>integrate when converting to cdf, outside the range provided for thepdf, must state that the value of the cdf is 0 before and 1 followingthe range; otherwise the cdf won't function outside of it, but the rangefor a cdf should support anything</p><h3 id="cdf---pdf">CDF -\> PDF</h3><p>derivative</p><h3 id="find-e-var-given-density-function">Find E, Var given density function</h3><ol><li>$E(X) = \int_{a}^{b} x f_{X}(x) dx$ over provided range$a \leq f_{X}(X) \leq b$</li><li><p>$Var(X) = E(X^{2}) - (E(X))^{2}$</p><ol><li>where $E(X^{2}) = \int_{a}^{b} x^{2} f_{X}(x) dx$</li></ol></li></ol><h3 id="piecewise-cdf-pdf-pdf---cdf">Piecewise CDF-\>PDF, PDF -\> CDF</h3><p>derivative, integral respectively of each function provided over eachrange</p><h3 id="joint-pdf-cdf">Joint PDF, CDF</h3><ol><li>Find marginal pdfs for each; for x, integrate by dy, and for yintegrate by dx</li><li>Set up integral for $E(XY)$; this should be some$\int_{c}^{d} \int_{a}^{b} x<em>y</em>f_{XY}(x,y) dx dy$ for bounds$a \leq x \leq b$ and $c \leq y \leq d$ . If bounds overlap,reference the relationship between them (i.e.$0 < y < x < 1 \implies a=y, b=1, c=0, d=1$), as the bounds of oneare dependent on the bounds for the other.</li><li>Find the <a href="#covariance">covariance</a>$Cov(X,Y) := E(XY) - E(X)E(Y)$. Use the marginal PDFs and integrateto find corresponding $E$, then follow the formula</li></ol><h3 id="finding-c-for-some-pdf-with-constant-to-solve-for">finding c for some pdf with constant to solve for</h3><h3 id="todo-combining-random-variables"><span class="todo TODO">TODO</span> Combining Random Variables</h3><p>i.e. test 2: 13, 14</p><h2 id="exam-3">Exam 3</h2><h3 id="use-normal-distribution-with-continuity-correction-to-estimate-probability-in-bounds">use normal distribution with continuity correction to estimate probability in bounds</h3><ol><li>find mean ($\mu$) and standard deviation ($\sigma$) of the providedscenario for the sample mean; note that$\sigma(\bar{x}) = \frac{\sigma}{\sqrt{n}}$</li><li><p>state that the normal approximation can be used if given "normalapproximation" or $n \geq 30$; "using Central Limit Theorem" ifusing this approximation, where$\bar{X} \tilde{=} N(\mu, \frac{\sigma}{\sqrt{n}})$</p><ol><li>TODO when to use $\frac{\sigma^{2}}{n}$?</li></ol></li><li>continuity correction: "round up or down" to the nearest 0.5 so thatthe interval encapsulates the intended population. i.e. if intervalis $\leq$, it's necessary to ensure interval will encapsulate upperand lower bound</li><li>apply normalcdf: $normalcdf(lower, upper, \mu, \sigma)$; where withsample mean, use $\sigma(\bar{x}) = \frac{\sigma}{\sqrt{n}}$instead. Use $10^{99}$ to replace upper and lower bounds (negativefor low) as needed to fill in provided open P(..) intervals.</li></ol><h3 id="solve-for-interval-given-resultant-probability">solve for interval given resultant probability</h3><ol><li>Investigate the interval; sketch it out relative to a normaldistribution. If it's two tailed, mark that</li><li>Finding the bound asked for:$invNorm(area before interval, \bar{x}, \sigma(\bar{x}))$ providessuch a bound.</li></ol><h3 id="probabilities-with-poisson-dist">probabilities with poisson dist.</h3><ol><li></li></ol><h3 id="finding-confidence-interval">finding confidence interval</h3><h3 id="maximum-likelihood-estimation">maximum likelihood estimation</h3><ol><li>Find $L(\theta) = \prod_{i=1}^{n}\f_{X}(x)$</li></ol><h2 id="exam-4">Exam 4</h2><h3 id="unbiasted-estimators-variance">unbiasted estimators variance</h3><p>Some $\hat{\theta}$ is an unbiased estimator for $\theta$ if$E(\hat{\theta}) = \theta$, so:</p><ol><li>Set up variance formula: ignore constants, and take variance of therandom variables used to calculate the new random variable</li><li>Substitute based on what's provided for these existing randomvariables; i.e. if $E(X) = E(Y) = \theta$, can substitute theexpected value of each there for theta when calculating</li><li>if original value is reached after evaluating, then it's an unbiasedestimator!</li></ol><h3 id="variance">variance</h3><p>examine the calculation for the random variable, squaring the constantsand taking the variance of the random variables used to calculate it</p><h3 id="find-convidence-interval">find convidence interval</h3><ol><li><p>T test</p><ol><li>State facts: S, df, $\alpha$</li><li>calculate $t_{\frac{\alpha}{2}, df}$ with$invT(1 - \alpha, df)$.</li><li>Find interval:$\bar{X} = t_{\frac{\alpha}{2}, df} \pm \frac{s}{\sqrt{n}}$; canuse TInterval</li></ol></li><li>Z test</li></ol><h3 id="statistical-test">statistical test</h3><h3 id="critical-values-and-errors">critical values and errors</h3><ol><li>find critical value in nonstandard form: typically$invNorm(accept-area, average, \frac{\sigma}{\sqrt{n}})$</li><li>Find type 1 error: typically $\alpha$</li><li>find type 2 error (given that real mean, $\mu(H_{a})$):$\beta = P(TypeII) = P(accept H_{0} | \mu = H_{a})$ -\>$normalcdf(b1, b2, \mu(H_{a}), \sigma(\bar{x}))$</li><li>Find the power of the test: $Power = 1 - \beta$</li></ol><h2 id="additional-material">Additional Material</h2></div></article></main><div class="git-hist-table"><table><tbody><tr><td>2023-02-22</td><td><a href="https://github.com/jakeisnt/wiki/blob/0514e759b834e8be98c86e544cda6a9450dd82ae/pages/prob-stat.md">0514e759</a></td></tr><tr><td>2023-02-22</td><td><a href="https://github.com/jakeisnt/wiki/blob/ea5044fc387f17e24dcab6f3873a541cd640bfa1/pages/prob-stat.md">ea5044fc</a></td></tr></tbody></table></div></div></body></html>